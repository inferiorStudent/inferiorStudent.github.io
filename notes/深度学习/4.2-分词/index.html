<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>4.2 分词 | Skeinz&#39;s page</title>
    
<link rel="stylesheet" href="/css/style.css">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          TeX: {
            extensions: ["amsmath.js", "cancel.js"],
          }
        });
    </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div class="container">
        <header class="post-navbar">
    <a href="/" class="nav-left">
        <img src="/img/avatar.jpg" alt="Avatar" class="nav-avatar">
        <span class="nav-title">上下求索</span>
    </a>

    <div class="nav-right">
        <div class="nav-item search-trigger">
            <img src="/img/icons/search.svg" alt="Search" class="search-icon">
            <span>请输入关键词搜索...</span>
        </div>

        <a href="/notes/" class="nav-link">笔记导航</a>

        <div class="nav-item theme-switcher">
            <img src="/img/icons/sun.svg" alt="Sun" class="sun-icon">
            <img src="/img/icons/moon.svg" alt="Moon" class="moon-icon">
        </div>
    </div>
</header>

<div id="search-modal" class="modal-overlay" style="display: none;">
    <div class="modal-content">
        <input type="text" id="search-input" placeholder="输入关键字搜索...">
        <div id="search-results"></div>
    </div>
</div>
<div class="post-layout">
  <aside class="post-sidebar">
    
    
      
      <nav>
          <div class="sidebar-top">
              <!-- <a href="/" class="back-to-home-link" style="display: flex; align-items: center">
                  <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 20 20"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m15 18l-6-6l6-6"/></svg>
                  <span>返回首页</span>
              </a> -->
              <h3>深度学习</h3>
          </div>
          <ul>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">1 机器学习</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.1-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/">1.1 评价指标</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.2-%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/">1.2 回归算法</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.3-%E5%86%B3%E7%AD%96%E6%A0%91/">1.3 决策树</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.4-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">1.4 支持向量机</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.5-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/">1.5 聚类算法</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.6-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">1.6 朴素贝叶斯</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">2 深度神经网络</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2.1-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99/">2.1 矩阵求导法则</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2.2-%E5%9B%9E%E5%BD%92/">2.2 训练回归模型</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2.3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">2.3 神经网络原理</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">3 计算机视觉基础</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.1-CNN/">3.1 卷积神经网络原理</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.2-LeNet/">3.2 LeNet</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.3-%E5%85%B6%E4%BB%96%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/">3.3 经典CNN模型</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.4-ResNet/">3.4 ResNet</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.5-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%BB%BB%E5%8A%A1/">3.5 计算机视觉任务</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">4 自然语言处理</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%BB%E5%8A%A1/">4.1 NLP发展</a>
                                      </li>
                                  
                                      <li class="sidebar-item active">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.2-%E5%88%86%E8%AF%8D/">4.2 分词</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.3-%E8%AF%8D%E5%B5%8C%E5%85%A5/">4.3 词嵌入</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86/">4.4 循环神经网络原理</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.5-LSTM%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/">4.5 RNN变体</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">4.6 注意力机制</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">5 预训练语言模型</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/5.1-Transformer/">5.1 Transformer</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/5.2-BERT/">5.2 BERT &amp; T5 &amp; ...</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/5.3-GPT%E7%B3%BB%E5%88%97/">5.3 GPT系列</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
          </ul>
      </nav>
    
  </aside>

  <article class="post-content">
    
      <div id="toc-container" class="toc-container collapsed">
        <button id="toc-toggle-btn" class="toc-toggle-btn">☰</button>
        <div class="toc-title">
            <span>文章目录</span>
        </div>
        <div class="toc-list">
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-BPE%E5%88%86%E8%AF%8D"><span class="toc-text">1 BPE分词</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-text">1.1 算法原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">1.2 代码实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-WordPiece"><span class="toc-text">2 WordPiece</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-text">2.1 算法原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">2.2 代码实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Unigram%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95"><span class="toc-text">3 Unigram分词算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-text">3.1 算法原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">3.2 代码实现</span></a></li></ol></li></ol>
        </div>
      </div>
    

    <h1>4.2 分词</h1>
    <!-- <div class="post-meta">
      
        <time class="post-date-tag" datetime="2025-10-29T08:24:23.864Z">
            发布于: 2025-10-29
        </time>
      
    </div> -->
    <div class="post-body">
        <p>token是NLP任务处理的最小单元，一个大语言模型所支持的所有token合起来就是该语言模型的<strong>词典</strong>。token除了人类的自然语言之外，为了在训练或者推理的过程中能够表示句子之间的分隔、段落信息等，语言模型还引入了特殊的token。比如我们使用大语言模型的过程中，模型会给出一段思考，然后再输出思考后的内容，思考的内容用<code>&lt;think&gt;</code>和<code>&lt;/think&gt;</code>包裹起来，就像HTML语言那样。除此之外，这里还列举出一些别的特殊的token：</p>
<ul>
<li><code>&lt;unk&gt;</code>：unknown，未在词表中出现的词（未登录词，Out Of Vocabulary, OOV）</li>
<li><code>&lt;bos&gt;</code>：begin of sequence</li>
<li><code>&lt;eos&gt;</code>：end of sequence</li>
</ul>
<p>由于不同的模型，其特殊token不尽相同，因此这里只列举一些。那么要实现一个语言模型，首要目标就是为该语言模型构造一个合适的词典，而且我们要保证想要生成的文本都能在该词典中找到对应的token组装起来这个过程叫做<font style="color: red;">Tokenizer</font>。这里介绍3个经典的分词算法。</p>
<h1 id="1-BPE分词">1 BPE分词</h1>
<p><em>字节对编码</em>*（Byte Pair Encoding，BPE）最早在1994年提出, 应用于数据压缩任务。2015年, 有人提出将其延申至 文本分词任务, 同时没有改变算法的名称。其核心思想是从字符开始，不断地把最常连续出现的“一对”字符/符号合并成一个新的、更长的符号。该算法被用于GPT大模型系列。</p>
<p>《牛津英语词典》收录了大约30万个英文单词，足以涵盖我们生活中绝大部分或者说所有的用语。如果将这30万个英文单词都作为token的话，那么大模型所需的算力就会非常庞大，导致推理速度变慢。我们在学习英语的过程中，单词和单词之间并非是完全割裂的，比如一个单词的过去式和过去分词是两个单词，但实际上和原型密切相关。因此只需要让模型学习原型，以及变形的后缀，通过词根来认识单词，就能减少词典的大小。</p>
<h2 id="1-1-算法原理">1.1 算法原理</h2>
<p>假设我们收集了一批语料，并统计了其中每个单词的出现次数（词频），<strong>同时规定词典的大小或者合并次数</strong>。我们的语料库中只包含low、lower、newest、widest这4个单词，统计的词频使用字典存储：<code>&#123;'low': 5, 'lower': 2, 'newest': 6, 'widest': 3&#125;</code>。<br>
<strong>第一步</strong>，首先，把所有词拆分成最基础的单元，也就是字符。为了区分单词的结尾，通常会加一个特殊的结束符，比如<code>&lt;/w&gt;</code>。那么统计的数据应该为<code>&#123;'low&lt;/w&gt;': 5, 'lower&lt;/w&gt;': 2, 'newest&lt;/w&gt;': 6, 'widest&lt;/w&gt;': 3&#125;</code>。<strong>初始词表</strong>，也是初始的vocabulary，就是所有出现过的单个字符：<code>[l, o, w, e, r, n, s, t, i, d, &lt;/w&gt;]</code>。<br>
<strong>第二步</strong>，找到最常见的邻居进行合并<br>
统计所有相邻的一对符号pair出现的总次数：</p>
<ul>
<li><code>(e, s)</code>: 在 new<strong>es</strong>t (6次) 和 wid<strong>es</strong>t (3次) 中出现，总共9次</li>
<li><code>(s, t)</code>: 在 newe<strong>st</strong> (6次) 和 wide<strong>st</strong> (3次) 中出现，总共9次</li>
<li><code>(t, &lt;/w&gt;)</code>: 在 newes<strong>t</strong> (6次) 和 wides<strong>t</strong> (3次) 中出现，总共9次</li>
<li><code>(l, o)</code>: 在 <strong>lo</strong>w (5次) 和 <strong>lo</strong>wer (2次) 中出现，总共7次</li>
<li><code>(o, w)</code>: 在 l<strong>ow</strong> (5次) 和 l<strong>ow</strong>er (2次) 中出现，总共7次</li>
<li>…更少的出现次数</li>
</ul>
<p>任选一个最高频的组合<code>es</code>，作为新的token加入到词表当中。同时数据变为：<code>&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, ' n e w es t &lt;/w&gt;': 6, 'w i d es t&lt;/w&gt;': 3&#125;</code>。接着重复上面的合并步骤，再次统计最常见的相邻对：</p>
<ul>
<li><code>(es, t)</code>: 在<code>n e w es t &lt;/w&gt;</code> (6次) 和 <code>w i d es t &lt;/w&gt;</code> (3次) 中出现，总共9次</li>
<li><code>(t, &lt;/w&gt;)</code>: 9次</li>
<li><code>(l, o)</code>: 7次</li>
<li><code>(o, w)</code>: 7次<br>
任选一个最高的组合<code>est</code>作为新的token加入到词表当中，数据变为：<code>&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, ' n e w est &lt;/w&gt;': 6, 'w i d est&lt;/w&gt;': 3&#125;</code>。</li>
</ul>
<p>重复上述步骤，可以继续依次得到新的token：<code>est&lt;/w&gt;</code>, <code>lo</code>, <code>low</code>… 直到词表的大小达到了我们规定的大小，这里规定16，那么我们最终得到的词表为：<code>[l, o, w, e, r, n, s, t, i, d, &lt;/w&gt;, es, est, est&lt;/w&gt;, lo, low]</code>。</p>
<p>那么对于一个输入的文本，如何用该词表进行表示？同样经过BPE分词过程，先按照字符进行拆分，再合并token，比如lowest就可以用<code>low</code>和<code>est&lt;/w&gt;</code>两个token来表示，用词典中的token下标表示分词结果为[15, 13]，其中下标从0开始。</p>
<h2 id="1-2-代码实现">1.2 代码实现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter, defaultdict<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BPE</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_merges</span>):<br>        <span class="hljs-variable language_">self</span>.num_merges = num_merges<br>        <span class="hljs-variable language_">self</span>.vocab = <span class="hljs-built_in">set</span>()<br>        <span class="hljs-variable language_">self</span>.merges = &#123;&#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_stats</span>(<span class="hljs-params">self, word_freqs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">int</span>]</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27; 统计相邻token对的词频</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            word_freqs = &#123;</span><br><span class="hljs-string">                &#x27;l o w &lt;/w&gt;&#x27;: 5,</span><br><span class="hljs-string">                &#x27;l o w e r &lt;/w&gt;&#x27;: 2,</span><br><span class="hljs-string">                &#x27;n e w e s t &lt;/w&gt;&#x27;: 6,</span><br><span class="hljs-string">                &#x27;w i d e s t &lt;/w&gt;&#x27;: 3</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        pairs = defaultdict(<span class="hljs-built_in">int</span>)<br>        <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> word_freqs.items():<br>            symbols = token.split()<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(symbols) - <span class="hljs-number">1</span>):<br>                pairs[(symbols[i], symbols[i + <span class="hljs-number">1</span>])] += freq<br>        <span class="hljs-keyword">return</span> pairs<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">merge_vocab</span>(<span class="hljs-params">self, pair: <span class="hljs-built_in">tuple</span>, word_freqs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">int</span>]:<br>        <span class="hljs-string">&#x27;&#x27;&#x27; 选择最高词频的token合并</span><br><span class="hljs-string">        Args</span><br><span class="hljs-string">            pair: 最高词频token对, 元组类型</span><br><span class="hljs-string">        Return (比如合并 es )</span><br><span class="hljs-string">            new_word_freqs = &#123;</span><br><span class="hljs-string">                &#x27;l o w &lt;/w&gt;&#x27;: 5,</span><br><span class="hljs-string">                &#x27;l o w e r &lt;/w&gt;&#x27;: 2,</span><br><span class="hljs-string">                &#x27;n e w es t &lt;/w&gt;&#x27;: 6,</span><br><span class="hljs-string">                &#x27;w i d es t &lt;/w&gt;&#x27;: 3</span><br><span class="hljs-string">            &#125;</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        new_word_freqs = &#123;&#125;<br>        bigram = <span class="hljs-string">&quot; &quot;</span>.join(pair)<br>        replacement = <span class="hljs-string">&quot;&quot;</span>.join(pair)<br>        <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> word_freqs.items():<br>            new_token = token.replace(bigram, replacement)<br>            new_word_freqs[new_token] = freq<br>        <span class="hljs-keyword">return</span> new_word_freqs<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_bpe</span>(<span class="hljs-params">self, corpus: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&#x27;&#x27;&#x27;根据语料库corpus 训练BPE分词器 并得到词典, 实际上这里没有加上&lt;/w&gt;标识符&#x27;&#x27;&#x27;</span><br>        word_freqs = Counter(corpus.split()) <span class="hljs-comment"># 这里并未考虑标点的处理</span><br>        word_freqs = &#123;<span class="hljs-string">&quot; &quot;</span>.join(<span class="hljs-built_in">list</span>(token)): freq <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> word_freqs.items()&#125;<br><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.num_merges):<br>            pairs = <span class="hljs-variable language_">self</span>.get_stats(word_freqs)<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> pairs:<br>                <span class="hljs-keyword">break</span><br>            best_pair = <span class="hljs-built_in">max</span>(pairs, key=pairs.get)   <span class="hljs-comment"># 得到元组</span><br>            <span class="hljs-variable language_">self</span>.merges[best_pair] = <span class="hljs-string">&quot;&quot;</span>.join(best_pair)<br>            word_freqs = <span class="hljs-variable language_">self</span>.merge_vocab(best_pair, word_freqs)<br>        <span class="hljs-variable language_">self</span>.vocab = <span class="hljs-built_in">set</span>(<span class="hljs-string">&quot; &quot;</span>.join(word_freqs.keys()).split())<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, text: <span class="hljs-built_in">str</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;根据vocab将文本编码为子词&#x27;&#x27;&#x27;</span><br>        words = text.split()<br>        encoded_words = []<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>            tokens = <span class="hljs-string">&quot; &quot;</span>.join(<span class="hljs-built_in">list</span>(word))<br>            <span class="hljs-keyword">for</span> pair, merge <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.merges.items():<br>                bigram = <span class="hljs-string">&quot; &quot;</span>.join(pair)<br>                tokens = tokens.replace(bigram, merge)<br>            encoded_words.append(tokens.split())<br>        <span class="hljs-keyword">return</span> encoded_words<br>    <br>corpus = <span class="hljs-string">&quot;low low low low low lower lower newest newest newest newest newest newest widest widest widest&quot;</span><br>bpe = BPE(num_merges=<span class="hljs-number">5</span>)<br>bpe.train_bpe(corpus)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;vocab: &quot;</span>, bpe.vocab)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;encoded word: &quot;</span>, bpe.encode(<span class="hljs-string">&quot;lowest&quot;</span>))<br><br><span class="hljs-string">&#x27;&#x27;&#x27;测试结果:</span><br><span class="hljs-string">vocab:  &#123;&#x27;r&#x27;, &#x27;ne&#x27;, &#x27;e&#x27;, &#x27;est&#x27;, &#x27;d&#x27;, &#x27;low&#x27;, &#x27;i&#x27;, &#x27;w&#x27;&#125;</span><br><span class="hljs-string">encoded word:  [[&#x27;low&#x27;, &#x27;est&#x27;]]</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>
<p>上面是简单的实现，你也可以看看源代码：<a target="_blank" rel="noopener" href="https://github.com/rsennrich/subword-nmt">BPE分词源码</a>。</p>
<h1 id="2-WordPiece">2 WordPiece</h1>
<p>WordPiece是GooGle于2016年的工作，其BERT模型使用了这种分词方法，是BPE分词的改进：BPE选择合并出现次数最多的pair，而WordPiece选择“价值”最高的pair。WordPiece的训练目标是找到一个词表，这个词表在用来切分语料库时，能使整个语料库的似然值Likelihood最大化。“价值”如何衡量？<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>score</mtext><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mtext>共同出现的次数</mtext></mrow><mrow><mi>a</mi><mtext>出现次数</mtext><mo>×</mo><mi>b</mi><mtext>出现次数</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{score}(a, b)=\frac{a, b\text{共同出现的次数}}{a\text{出现次数}\times b\text{出现次数}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">score</span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3355em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord text mtight"><span class="mord cjk_fallback mtight">出现次数</span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">b</span><span class="mord text mtight"><span class="mord cjk_fallback mtight">出现次数</span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">b</span><span class="mord text mtight"><span class="mord cjk_fallback mtight">共同出现的次数</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。分数越高，说明<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a,b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span></span></span></span>绑定程度很高。</p>
<blockquote>
<p>[!WARNING]</p>
<p>Google从未开源其WordPiece训练算法的实现，因此以下内容是根据已发表的文献做出的猜测，不一定完全正确。</p>
</blockquote>
<h2 id="2-1-算法原理">2.1 算法原理</h2>
<p>⭕<strong>训练算法</strong></p>
<p>这里仍然采用第1节的例子来说明算法过程，不需要加上特殊的结束符：<code>&#123;'low': 5, 'lower': 2, 'newest': 6, 'widest': 3&#125;</code>。<br>
<strong>第一步</strong>，BERT中使用前缀<code>##</code>来标记子词将数据按照最小符号拆分：<code>&#123;'l ##o ##w': 5, 'l ##o ##w ##e ##r': 2, 'n ##e ##w ##e ##s ##t': 6, 'w ##i ##d ##e ##s ##t': 3&#125;</code>，，因此词表表示为：<code>&#123; l, n, w, ##o, ##w, ##e, ##r, ##n, ##s, ##t, ##i, ##d &#125;</code>。<br>
<strong>第二步</strong>，根据统计的词频来计算分数，选择分数最高的那一个：<code>score(w, ##i) = 1 / 3</code>.<br>
<strong>第三步</strong>，合并：将<code>lo</code>添加到词表中，并更新数据：<code>&#123;'l ##o ##w': 5, 'l ##o ##w ##e ##r': 2, 'n ##e ##w ##e ##s ##t': 6, 'wi ##d ##e ##s ##t': 3&#125;</code>。注意合并的时候将后面词根的<code>##</code>去掉。</p>
<p>重复上面的第二、三步，直到词表达到已设置的最大值。</p>
<p>⭕<strong>分词算法</strong></p>
<p><font style="color: red;">WordPiece只保存最终词汇表，而不保存学习到的合并规则。</font>从要分词的单词开始，WordPiece 查找词汇表中存在的<strong>最长的子词</strong>，然后在其上进行拆分。比如生成词表<code>['l', '##o', '##w', '##e', '##r', 'n', '##s', '##t', 'w', '##i', '##d', 'wi', 'wid', 'lo', '##st']</code>，对于单词lowest：</p>
<ul>
<li><code>l</code>是否在词表中？是，进行下一步</li>
<li><code>lo</code>是否在词表中？是，进行下一步</li>
<li><code>low</code>是否在词表中？否，lowest第一个token则为<code>lo</code>，剩下的部分为<code>##west</code></li>
</ul>
<p>对剩下部分进行同样的操作，最终单词被分解为<code>[lo, ##w, ##e, ##st]</code>。</p>
<h2 id="2-2-代码实现">2.2 代码实现</h2>
<p>虽然不一定完全正确，但根据上述猜测的算法实现该分词器也算是练手了。注意这里暂时只实现训练的算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WordPiece</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size</span>):<br>        <span class="hljs-variable language_">self</span>.vocab_size = vocab_size<br>        <span class="hljs-comment"># 在BERT中还添加了一些特殊的符号</span><br>        <span class="hljs-variable language_">self</span>.vocab = [<span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>]<br>        <span class="hljs-variable language_">self</span>.splits = &#123;&#125;<br>        <span class="hljs-variable language_">self</span>.word_freqs = defaultdict(<span class="hljs-built_in">int</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_stats</span>(<span class="hljs-params">self, corpus: <span class="hljs-built_in">str</span></span>):<br>        word_freqs = defaultdict(<span class="hljs-built_in">int</span>)<br>        words = corpus.split()<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>            word_freqs[word] += <span class="hljs-number">1</span><br>        <br>        alphabet = []<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_freqs.keys():<br>            <span class="hljs-keyword">if</span> word[<span class="hljs-number">0</span>] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> alphabet:<br>                alphabet.append(word[<span class="hljs-number">0</span>])<br>            <span class="hljs-keyword">for</span> letter <span class="hljs-keyword">in</span> word[<span class="hljs-number">1</span>:]:<br>                <span class="hljs-keyword">if</span> <span class="hljs-string">f&quot;##<span class="hljs-subst">&#123;letter&#125;</span>&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> alphabet:<br>                    alphabet.append(<span class="hljs-string">f&quot;##<span class="hljs-subst">&#123;letter&#125;</span>&quot;</span>)<br>        <br>        <span class="hljs-variable language_">self</span>.vocab = <span class="hljs-variable language_">self</span>.vocab + alphabet.copy()<br>        <span class="hljs-comment"># &#123;&#x27;low&#x27;: [&#x27;l&#x27;, &#x27;##o&#x27;, &#x27;##w&#x27;]&#125;</span><br>        <span class="hljs-variable language_">self</span>.splits = &#123;word: [c <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">f&quot;##<span class="hljs-subst">&#123;c&#125;</span>&quot;</span> <span class="hljs-keyword">for</span> i, c <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word)] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_freqs.keys()&#125;<br>        <span class="hljs-variable language_">self</span>.word_freqs = word_freqs<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_pair_scores</span>(<span class="hljs-params">self, word_freqs: defaultdict, splits: <span class="hljs-built_in">dict</span></span>) -&gt; <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">tuple</span>, <span class="hljs-built_in">float</span>]:<br>        letter_freqs = defaultdict(<span class="hljs-built_in">int</span>)<br>        pair_freqs = defaultdict(<span class="hljs-built_in">int</span>)<br>        <span class="hljs-keyword">for</span> word, freq <span class="hljs-keyword">in</span> word_freqs.items():<br>            split = splits[word]<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(split) == <span class="hljs-number">1</span>:<br>                letter_freqs[split[<span class="hljs-number">0</span>]] += freq<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(split) - <span class="hljs-number">1</span>):<br>                pair = (split[i], split[i + <span class="hljs-number">1</span>])<br>                letter_freqs[split[i]] += freq<br>                pair_freqs[pair] += freq<br>            letter_freqs[split[-<span class="hljs-number">1</span>]] += freq<br>        <br>        scores = &#123;pair: freq / (letter_freqs[pair[<span class="hljs-number">0</span>]] * letter_freqs[pair[<span class="hljs-number">1</span>]]) <span class="hljs-keyword">for</span> pair, freq <span class="hljs-keyword">in</span> pair_freqs.items()&#125;<br>        <span class="hljs-keyword">return</span> scores<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">merge_pair</span>(<span class="hljs-params">self, pair: <span class="hljs-built_in">tuple</span>, splits: <span class="hljs-built_in">dict</span></span>):<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.word_freqs:<br>            split = splits[word]<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(split) == <span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(split) - <span class="hljs-number">1</span>):<br>                <span class="hljs-keyword">if</span> split[i] == pair[<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> split[i + <span class="hljs-number">1</span>] == pair[<span class="hljs-number">1</span>]:<br>                    merge = pair[<span class="hljs-number">0</span>] + pair[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>:] <span class="hljs-keyword">if</span> pair[<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;##&#x27;</span>) <span class="hljs-keyword">else</span> pair[<span class="hljs-number">0</span>] + pair[<span class="hljs-number">1</span>]<br>                    split = split[:i] + [merge] + split[i + <span class="hljs-number">2</span> :]<br>                <span class="hljs-keyword">else</span>:<br>                    i += <span class="hljs-number">1</span><br>            splits[word] = split<br>        <span class="hljs-keyword">return</span> splits<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, corpus: <span class="hljs-built_in">str</span></span>):<br>        <span class="hljs-variable language_">self</span>.get_stats(corpus)<br>        <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.vocab) &lt; <span class="hljs-variable language_">self</span>.vocab_size:<br>            scores = <span class="hljs-variable language_">self</span>.compute_pair_scores(<span class="hljs-variable language_">self</span>.word_freqs, <span class="hljs-variable language_">self</span>.splits)<br>            best_pair = <span class="hljs-built_in">max</span>(scores, key=scores.get)<br>            <span class="hljs-variable language_">self</span>.splits = <span class="hljs-variable language_">self</span>.merge_pair(best_pair, <span class="hljs-variable language_">self</span>.splits)<br>            new_token = (best_pair[<span class="hljs-number">0</span>] + best_pair[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>:] <span class="hljs-keyword">if</span> best_pair[<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&quot;##&quot;</span>) <span class="hljs-keyword">else</span> best_pair[<span class="hljs-number">0</span>] + best_pair[<span class="hljs-number">1</span>])<br>            <span class="hljs-variable language_">self</span>.vocab.append(new_token)<br>                <br><br>corpus = <span class="hljs-string">&quot;low low low low low lower lower newest newest newest newest newest newest widest widest widest&quot;</span><br>wp = WordPiece(<span class="hljs-number">20</span>)<br>wp.train(corpus)<br><span class="hljs-built_in">print</span>(wp.vocab)<br><br><span class="hljs-string">&#x27;&#x27;&#x27; output   </span><br><span class="hljs-string">[&#x27;[PAD]&#x27;, &#x27;[UNK]&#x27;, &#x27;[CLS]&#x27;, &#x27;[SEP]&#x27;, &#x27;[MASK]&#x27;, &#x27;l&#x27;, &#x27;##o&#x27;, &#x27;##w&#x27;, &#x27;##e&#x27;, &#x27;##r&#x27;, &#x27;n&#x27;, &#x27;##s&#x27;, &#x27;##t&#x27;, &#x27;w&#x27;, &#x27;##i&#x27;, &#x27;##d&#x27;, &#x27;wi&#x27;, &#x27;wid&#x27;, &#x27;lo&#x27;, &#x27;##st&#x27;]</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>
<h1 id="3-Unigram分词算法">3 Unigram分词算法</h1>
<p>和BPE / WordPiece算法相比，Unigram算法是从一个非常大的词表开始删减无用的单元。其出发点是概率，它假设所有的子词（subword）都是独立出现的，和n-gram中的unigram一样的假设。它的训练目标是：找到一个最优的词表，使得用这个词表来切分语料库时，整个语料库的“总概率”最大。</p>
<h2 id="3-1-算法原理">3.1 算法原理</h2>
<p>在训练过程中，对一个单词low有多种切分方式：</p>
<ul>
<li><code>[l, o, w]</code>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>low</mtext><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>l</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>o</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>w</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\text{low})=P(\text{l})\cdot P(\text{o})\cdot P(\text{w})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">low</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">l</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">o</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">w</span></span><span class="mclose">)</span></span></span></span></li>
<li><code>[l, ow]</code>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>low</mtext><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>l</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>ow</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\text{low})=P(\text{l})\cdot P(\text{ow})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">low</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">l</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">ow</span></span><span class="mclose">)</span></span></span></span></li>
<li><code>[lo, w]</code>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>low</mtext><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>lo</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>w</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\text{low})=P(\text{lo})\cdot P(\text{w})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">low</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">lo</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">w</span></span><span class="mclose">)</span></span></span></span></li>
</ul>
<p>训练词表的过程是EM算法+删减（Pruning）</p>
<ol>
<li>首先初始化一个词表，该vocab可以包含所有预切分好的单词、所有的子字符串、以及所有单个字符，即对于一个单词<code>low</code>来说，它的所有子词全都包含进去。</li>
<li>采用EM算法迭代优化
<ul>
<li>E-Step（期望）：假设当前的词表和概率是固定的。使用Viterbi算法，为语料库中的每一个词找到当前最优的切分方式。</li>
<li>M-Step（最大化）：统计E-Step中所有“最优切分”的结果，重新计算词表中每个子词的出现次数和概率 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>token</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\text{token})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">token</span></span><span class="mclose">)</span></span></span></span>。</li>
</ul>
</li>
<li>删减：从词表中删掉某个子词，语料库总概率会下降多少。
<ul>
<li>如果删除low导致总概率下降得非常多，low必须分为更细的子词，如<code>[l, o, w]</code>，说明<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>l</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>o</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>w</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\text{l})\cdot P(\text{o})\cdot P(\text{w})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">l</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">o</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">w</span></span><span class="mclose">)</span></span></span></span>远小于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>low</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\text{low})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">low</span></span><span class="mclose">)</span></span></span></span>，low就是一个有价值的子词，应该保留。反之就是一个冗余的子词。</li>
</ul>
</li>
</ol>
<p>循环上述2、3步骤，直到词表减小到我们设定的大小。</p>
<p>在分词过程中，Unigram会使用Viterbi算法 找到那个乘积概率最大的切分路径。</p>
<blockquote>
<p>[!INFO]</p>
<p>Unigram常用于SentencePiece，SentencePiece是AlBERT、T5、mBART、Big Bird和XLNet等模型使用的无监督分词器。</p>
</blockquote>
<p><u>举一个🌰说明SentencePiece算法流程</u>，Unigram的实现通常会用一个特殊符号（如下划线/空格）来标记词的开头，这里为了简化，暂时省略。仍然使用上面相同的语料库<code>&#123;'low': 5, 'lower': 2, 'newest': 6, 'widest': 3&#125;</code>，初始化大词表：<code>&#123; l, o, w, e, r, n, s, t, i, d, lo, ow, we, er, ne, ew, ..., low, lower, newest, widest &#125;</code>，计算所有的子词词频（这里不想算了）。</p>
<p>暂时空着，先看看官网的解释：<a target="_blank" rel="noopener" href="https://hugging-face.cn/learn/nlp-course/chapter6/7#pt">hugging face: unigram</a>。</p>
<h2 id="3-2-代码实现">3.2 代码实现</h2>
<p>懒得写了，暂时空着</p>

    </div>
    <script>
    document.addEventListener('DOMContentLoaded', () => {
        const codeBlocks = document.querySelectorAll('figure.highlight');
    
        codeBlocks.forEach(block => {
            // 1. 获取语言标签
            const lang = block.classList.contains('highlight') ? 
                         block.classList[1] : '';
    
            // 2. 创建一个容器来包裹语言标签和复制按钮
            const toolbar = document.createElement('div');
            toolbar.className = 'highlight-toolbar';
    
            // 3. 创建语言标签
            if (lang) {
                const langLabel = document.createElement('span');
                langLabel.className = 'highlight-lang';
                langLabel.textContent = lang.toUpperCase();
                toolbar.appendChild(langLabel);
            }
    
            // 4. 创建复制按钮
            const copyButton = document.createElement('button');
            copyButton.className = 'highlight-copy-btn';
            copyButton.textContent = '复制';
            
            copyButton.addEventListener('click', () => {
                // 优先选择 .code 元素（针对有行号的表格），如果没有则选择 pre 元素
                const codeElement = block.querySelector('.code') || block.querySelector('pre');
                if (codeElement) {
                    navigator.clipboard.writeText(codeElement.textContent).then(() => {
                        copyButton.textContent = '已复制!';
                        setTimeout(() => {
                            copyButton.textContent = '复制';
                        }, 2000);
                    }).catch(err => {
                        console.error('复制失败: ', err);
                    });
                }
            });
            toolbar.appendChild(copyButton);
    
            // 5. 将工具栏插入到代码块中
            block.appendChild(toolbar);
        });
    });
</script>
  </article>
</div>

    </div>
    
<script src="/js/script.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>