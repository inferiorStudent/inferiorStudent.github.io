<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>5.2 BERT &amp; T5 &amp; ... | Skeinz&#39;s page</title>
    
<link rel="stylesheet" href="/css/style.css">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          TeX: {
            extensions: ["amsmath.js", "cancel.js"],
          }
        });
    </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div class="container">
        <header class="post-navbar">
    <a href="/" class="nav-left">
        <img src="/img/avatar.jpg" alt="Avatar" class="nav-avatar">
        <span class="nav-title">上下求索</span>
    </a>

    <div class="nav-right">
        <div class="nav-item search-trigger">
            <img src="/img/icons/search.svg" alt="Search" class="search-icon">
            <span>请输入关键词搜索...</span>
        </div>

        <a href="/notes/" class="nav-link">笔记导航</a>

        <div class="nav-item theme-switcher">
            <img src="/img/icons/sun.svg" alt="Sun" class="sun-icon">
            <img src="/img/icons/moon.svg" alt="Moon" class="moon-icon">
        </div>
    </div>
</header>

<div id="search-modal" class="modal-overlay" style="display: none;">
    <div class="modal-content">
        <input type="text" id="search-input" placeholder="输入关键字搜索...">
        <div id="search-results"></div>
    </div>
</div>
<div class="post-layout">
  <aside class="post-sidebar">
    
    
      
      <nav>
          <div class="sidebar-top">
              <!-- <a href="/" class="back-to-home-link" style="display: flex; align-items: center">
                  <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 20 20"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m15 18l-6-6l6-6"/></svg>
                  <span>返回首页</span>
              </a> -->
              <h3>深度学习</h3>
          </div>
          <ul>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">1 机器学习</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.1-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/">1.1 评价指标</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.2-%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/">1.2 回归算法</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.3-%E5%86%B3%E7%AD%96%E6%A0%91/">1.3 决策树</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.4-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">1.4 支持向量机</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.5-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/">1.5 聚类算法</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/1.6-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">1.6 朴素贝叶斯</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">2 深度神经网络</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2.1-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E6%B3%95%E5%88%99/">2.1 矩阵求导法则</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2.2-%E5%9B%9E%E5%BD%92/">2.2 训练回归模型</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2.3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">2.3 神经网络原理</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">3 计算机视觉基础</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.1-CNN/">3.1 卷积神经网络原理</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.2-LeNet/">3.2 LeNet</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.3-%E5%85%B6%E4%BB%96%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/">3.3 经典CNN模型</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.4-ResNet/">3.4 ResNet</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3.5-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%BB%BB%E5%8A%A1/">3.5 计算机视觉任务</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">4 自然语言处理</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%BB%E5%8A%A1/">4.1 NLP发展</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.2-%E5%88%86%E8%AF%8D/">4.2 分词</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.3-%E8%AF%8D%E5%B5%8C%E5%85%A5/">4.3 词嵌入</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.4-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86/">4.4 循环神经网络原理</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.5-LSTM%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93/">4.5 RNN变体</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/4.6-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">4.6 注意力机制</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">5 预训练语言模型</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/5.1-Transformer/">5.1 Transformer</a>
                                      </li>
                                  
                                      <li class="sidebar-item active">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/5.2-BERT/">5.2 BERT &amp; T5 &amp; ...</a>
                                      </li>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/notes/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/5.3-GPT%E7%B3%BB%E5%88%97/">5.3 GPT系列</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
          </ul>
      </nav>
    
  </aside>

  <article class="post-content">
    
      <div id="toc-container" class="toc-container collapsed">
        <button id="toc-toggle-btn" class="toc-toggle-btn">☰</button>
        <div class="toc-title">
            <span>文章目录</span>
        </div>
        <div class="toc-list">
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0-ELMo"><span class="toc-text">0 ELMo</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-BERT"><span class="toc-text">1 BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="toc-text">1.1 词嵌入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87"><span class="toc-text">1.2 预训练目标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1-MLM"><span class="toc-text">1.2.1 MLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-2-NSP"><span class="toc-text">1.2.2 NSP</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E5%BE%AE%E8%B0%83"><span class="toc-text">1.3 微调</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-T5"><span class="toc-text">2 T5</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Pre-train-Span-Corruption"><span class="toc-text">2.1 Pre-train: Span Corruption</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Ablation-Study"><span class="toc-text">2.2 Ablation Study</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9E%8B"><span class="toc-text">3 其他模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-BERT%E6%94%B9%E8%BF%9B"><span class="toc-text">3.1 BERT改进</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-BART"><span class="toc-text">3.2 BART</span></a></li></ol></li></ol>
        </div>
      </div>
    

    <h1>5.2 BERT &amp; T5 &amp; ...</h1>
    <!-- <div class="post-meta">
      
        <time class="post-date-tag" datetime="2025-11-04T06:53:05.786Z">
            发布于: 2025-11-04
        </time>
      
    </div> -->
    <div class="post-body">
        <p><font style="color: red;"><strong>后续完善修改ELMo</strong></font></p>
<h1 id="0-ELMo">0 ELMo</h1>
<p>2018年3月Washington University提出ELMo（Embeddings from Language Models），<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a>。</p>
<p>ELMo是一种深度上下文化的词表示模型，它能够捕捉单词使用中的复杂特征（如语法和语义），并根据不同的语言环境来调整这些特征，以处理多义性问题。ELMo的词向量是通过预训练的深度双向语言模型（biLM）的内部状态来学习得到的。这些词向量可以轻松地集成到现有模型中，显著提高多个自然语言处理任务的性能，包括问答、文本蕴含和情感分析等。</p>
<p>ELMo模型的核心是一个双向的长短期记忆网络（LSTM），它由两层LSTM组成，分别处理输入句子的前向和后向上下文。模型的输入是字符级的表示，这些字符表示通过卷积神经网络（CNN）进行编码，然后通过池化层和高速网络（highway networks）进行处理，最终生成每个单词的向量表示。</p>
<p>在下游自然语言处理任务中，ELMo的词向量通常是通过任务相关的线性组合来学习得到的，这使得ELMo能够提供上下文敏感的词表征。这种方法允许模型根据具体任务调整词向量的权重，从而更好地捕捉词在不同语境下的含义。</p>
<p>ELMo的主要优势在于其上下文相关性和深度特性。由于ELMo基于字符的表示，它能够有效地处理未知词（OOV）问题，并且不需要大规模的词汇表。此外，ELMo的预训练模型和参数可以在多种语言上应用，为多语言任务提供了强大的支持。</p>
<h1 id="1-BERT">1 BERT</h1>
<p>2018年10月，Google AI研究院提出的一种预训练模型BERT，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">Bidirectional Encoder Representation from Transformers</a>。BERT是一个<font style="background-color: rgba(255, 255, 51, 0.3); padding: 1px 2px;">Encoder-only</font>的结构，也就是只有Transformer的Encoder模块。论文中展现了两个经典版本，仅仅在参数数量有所区别：</p>
<ul>
<li>BERT-base：12层Encoder，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">d_{model}=768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">768</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">h=12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">12</span></span></span></span>，总参数量约110M；</li>
<li>BERT-large：24层Encoder，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">d_{model}=1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1024</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">h=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">16</span></span></span></span>，总参数量约340M。</li>
</ul>
<p>BERT证明了可以先用海量无标注数据训练一个超强的通用模型，然后只用少量标注数据就能在下游任务上达到SOTA。这极大降低了NLP任务的门槛。</p>
<h2 id="1-1-词嵌入">1.1 词嵌入</h2>
<p>BERT的词嵌入分3个部分：</p>
<ul>
<li>
<p>Token Embedding：使用WordPiece分词；加入特殊标记<code>[CLS]</code>（放在句首，表示整个句子的语义，是classes的缩写，用于分类任务）和<code>[SEP]</code>（用于分隔两个句子，如在问答或推理任务中）；</p>
</li>
<li>
<p>Segment Embedding：用来区分输入是一句话还是两句话；<br>
比如对于两句话，分词并加入特殊标记后得到<code>['CLS', '天气', '很', '不错', '[SEP]', '太阳', '非常', '耀眼', '[SEP]']</code>，前一句话用<code>0</code>来代表所有的token，包括<code>[CLS]</code>，后一句话用<code>0</code>来表示所有的token，得到一个分段序列<code>[0, 0, 0, 0, 0, 1, 1, 1, 1]</code>，然后在词汇表中查询0和1对应的词嵌入，构造出一个维度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="double-struck">R</mi><mrow><mi>B</mi><mo>×</mo><mi>S</mi><mo>×</mo><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^{B\times S\times d_{model}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>的分段嵌入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">E</mi><mrow><mi>s</mi><mi>e</mi><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\boldsymbol E_{seg}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9722em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.05451em;">E</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">se</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>。如果只有一句话，那么全用<code>0</code>来表示。</p>
</li>
<li>
<p>Position Embedding：和Transformer不同的是，位置嵌入是一个<strong>可学习</strong>的参数。</p>
</li>
</ul>
<p>那么最终的输入为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">E</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>=</mo><msub><mi mathvariant="bold-italic">E</mi><mrow><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi></mrow></msub><mo>+</mo><msub><mi mathvariant="bold-italic">E</mi><mrow><mi>s</mi><mi>e</mi><mi>g</mi></mrow></msub><mo>+</mo><mstyle mathcolor="red"><msub><mi mathvariant="bold-italic">E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mstyle></mrow><annotation encoding="application/x-tex">\boldsymbol E_{in} = \boldsymbol E_{token} + \boldsymbol E_{seg} + {\color{red} \boldsymbol E_{pos}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8361em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.05451em;">E</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">in</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8361em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.05451em;">E</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9722em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.05451em;">E</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">se</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9722em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord" style="color:red;"><span class="mord" style="color:red;"><span class="mord" style="color:red;"><span class="mord boldsymbol" style="margin-right:0.05451em;color:red;">E</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight" style="color:red;"><span class="mord mtight" style="color:red;"><span class="mord mathnormal mtight" style="color:red;">p</span><span class="mord mathnormal mtight" style="color:red;">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>.</p>
<h2 id="1-2-预训练目标">1.2 预训练目标</h2>
<p>BERT采用了自监督预训练（self-supervised pre-train），不需要人工标注，直接在海量的纯文本（维基百科+书籍语料库）上进行训练，并且设计了2个任务同时训练模型。</p>
<h3 id="1-2-1-MLM">1.2.1 MLM</h3>
<p>掩码语言模型（Masked Language Model），俗称完形填空。传统的语言模型是自回归式的，预测下一个词只能将注意力放在生成词之前，也就是该词的左边。而BERT的做法是<strong>随机</strong>遮挡句子中的某些词，让模型利用上下文来预测被遮挡的词是什么，不仅仅局限于“左边”，因此称之为“双向的”（Bidirectional）。具体的步骤是随机选择输入序列中15%的token，对于被选中的token，采取如下做法：</p>
<ol>
<li>80%的概率替换为<code>[MASK]</code>标记；</li>
<li>10%的概率替换为一个随机token，这是为了让模型保持警惕，不要只盯着被Mask掉的词，要知道非Mask词也有可能是错的。这可以理解为对正确答案进行一定的扰动，添加噪音，（防止过拟合，）错误答案仅占总token的1.5%，负面影响没那么明显；</li>
<li>10%的概率保持不变。</li>
</ol>
<p>损失函数只计算被Mask掉的那一部分token的交叉熵损失函数。具体来说，Encoder输出<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="double-struck">R</mi><mrow><mi>B</mi><mo>×</mo><mi>S</mi><mo>×</mo><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^{B\times S\times d_{model}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>大小的张量，使用一个全连接层将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>映射到词汇表大小<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>，取出被Mask掉的位置对应的logits做交叉熵损失。</p>
<blockquote>
<p>[!PERPLEX]</p>
<p>为什么只计算被Mask掉的那一部分token的Cross Entropy？BERT预训练模型的目的是理解上下文，训练其根据上下文进行推理的能力，而不是做文本生成的任务，这和Transformer完全不同。如果将非Mask的token的损失添加进来，模型能够直接看到输入进来token，会发现直接照抄已有的token就能降低损失。</p>
</blockquote>
<h3 id="1-2-2-NSP">1.2.2 NSP</h3>
<p>下一句预测（Next Sentence Prediction）是为了让模型理解句子和句子之间的关系，这对问答系统QA和自然语言推理NLI有很大的作用。具体做法是用<code>[SEP]</code>将两个句子A和B分隔开，正如在Segment Embedding中所描述的那样。</p>
<ol>
<li>50%的情况：B是A的下一句，标签为IsNext；</li>
<li>50%的情况：B是语料库中随机抽取的依据，标签为NotNext。</li>
</ol>
<p>最后取出<code>[CLS]</code>位置的输出向量，接一个二分类层来判断B是否是A的下文。采用二分类交叉熵损失函数。</p>
<h2 id="1-3-微调">1.3 微调</h2>
<p>预训练好的BERT模型已经具备理解语言的基础能力，当我们处理具体的下游任务时，只需要在BERT原有的结构上添加特定的输出层，通过特定任务的训练使BERT的权重参数进行微小的改变，使之适用于特定的任务：</p>
<ul>
<li>单句分类任务，如情感分析：输入<code>[CLS]</code>+句子+<code>[SEP]</code>，取出<code>[CLS]</code>位置对应的输出向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">C</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></msup></mrow><annotation encoding="application/x-tex">\boldsymbol C\in\mathbb{R}^{d_{model}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7252em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.06979em;">C</span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>，加一个线性层做一个多分类任务；</li>
<li>序列标注任务，如NER：输入<code>[CLS]</code>+句子+<code>[SEP]</code>，取出每一个输出token对应的向量，对每一个向量进行分类；</li>
<li>问答任务，如SQuAD：输入<code>[CLS]</code>+句子+<code>[SEP]</code>+包含答案的文本段落+<code>[SEP]</code>，目标是找到答案在段落中的起始位置和结束位置，具体就是学习两个向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">S</mi></mrow><annotation encoding="application/x-tex">\boldsymbol S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.05382em;">S</span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">E</mi></mrow><annotation encoding="application/x-tex">\boldsymbol E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.05451em;">E</span></span></span></span></span></span>，计算段落中每个token与这两个向量的点积，最大的位置即为答案区间。</li>
</ul>
<h1 id="2-T5">2 T5</h1>
<p>文本到文本（Text-to-Text Transfer Transformer）由Google于2020年年初发布，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>。它不再针对不同的任务设计不同的模型结构或输出层，而是提出“万物皆可text-to-text”，具有里程碑意义。该论文还贡献了一个清洗得非常干净的数据集Colossal Clean Crawled Corpus（简称C4），作者对Common Crawl（全网爬虫）进行了极其严格的清洗，得到了750G的纯净数据集。</p>
<p>虽然T5在机器翻译和摘要任务上表现极佳，但其模型本质还是Seq2Seq，导致推理速度慢（分类任务不如Encoder-only），而且非自回归生成（预训练目标仍然是做完形填空）导致生成内容并不高效自然（自回归生成不如Decoder-only）。</p>
<p>在T5之前，处理不同的NLP任务需要不同的输出层，比如情感分类的分类输出、回归任务（文本相似度）的分数输出以及机器翻译的文本输出。T5规定所有的NLP任务都是文本输入、文本输出。</p>
<table>
<thead>
<tr>
<th><strong>任务类型</strong></th>
<th><strong>输入给 T5 的内容 (Prompt)</strong></th>
<th><strong>T5 的目标输出 (Text)</strong></th>
<th><strong>传统模型的做法</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>机器翻译</strong></td>
<td><code>&quot;translate English to German: That is good.&quot;</code></td>
<td><code>&quot;Das ist gut.&quot;</code></td>
<td>Seq2Seq</td>
</tr>
<tr>
<td><strong>情感分析</strong></td>
<td><code>&quot;sst2 sentence: The movie is terrible.&quot;</code></td>
<td><code>&quot;negative&quot;</code></td>
<td>此时输出通常是类别索引 <code>0</code></td>
</tr>
<tr>
<td><strong>句子相似度</strong></td>
<td><code>&quot;stsb sentence1: ... sentence2: ...&quot;</code></td>
<td><code>&quot;3.8&quot;</code></td>
<td>此时输出是一个浮点数 <code>3.8</code></td>
</tr>
<tr>
<td><strong>阅读理解</strong></td>
<td><code>&quot;question: ... context: ...&quot;</code></td>
<td><code>&quot;Washington D.C.&quot;</code></td>
<td>预测 Start/End 索引位置</td>
</tr>
</tbody>
</table>
<p>这意味着可以使用同一个模型、同一个损失函数、同一套超参数完成所有NLP任务、解决所有问题！T5沿用了标准的Transformer架构，仅仅在位置编码上有所不同，即T5使用了<strong>相对位置编码</strong>（Relative Positional Embeddings）：只关心词A和词B之间隔了多少词，而非句子序列中的绝对位置。这使得模型在<u>推理</u>时对长短不一的序列泛化能力更强。<font style="color: red;"><strong>补充相对位置编码的具体例子</strong></font></p>
<h2 id="2-1-Pre-train-Span-Corruption">2.1 Pre-train: Span Corruption</h2>
<p>T5提出了一种更适合生成任务的掩码方式：片段破坏（Span Corruption）。假设原始的句子是</p>
<p style="text-align:center">The cute cat is playing in the park happily.</p>
<p>T5能够Mask掉连续的片段，每段用一个特殊的哨兵标记（Sentinel token）替代，假设Mask后的Encoder输入为</p>
<p style="text-align:center">The cute <code><<X>X></code> in the <code><<X>Y></code>.</p>
<p>这里<code>&lt;X&gt;</code>替代了“cat is playing”，<code>&lt;Y&gt;</code>替代了“park happily”。输出就只需要关心被Mask掉的部分即可。这样训练了模型对连贯的短语、词组等不同长度文本的生成能力。Decoder的输出需要生成被Mask掉的内容，并且用对应的哨兵标记分隔</p>
<p style="text-align:center"><code><<X>A></code> cat is playing <code><<X>B></code> park happily<code><<X>C></code></p>
<h2 id="2-2-Ablation-Study">2.2 Ablation Study</h2>
<p>消融实验可以理解为一种控制变量的方法，研究单一变量对模型性能的影响。论文从以下3个方面进行了比较：</p>
<ol>
<li>架构：Encoder-only、Decoder-only、Encoder-Decoder✅</li>
<li>预训练目标：单字Mask、Span Corruption✅</li>
<li>参数更新策略：微调所有参数✅、只微调部分层</li>
</ol>
<h1 id="3-其他模型">3 其他模型</h1>
<h2 id="3-1-BERT改进">3.1 BERT改进</h2>
<p>在BERT发布以后，一些模型通过改进训练方法、数据量或参数效率，把BERT的性能推到了极限，但并没有改变其Encoder-only的结构。</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>发布信息</th>
<th>改进</th>
<th>优劣</th>
</tr>
</thead>
<tbody>
<tr>
<td>RoBERTa</td>
<td>Facebook AI，2019</td>
<td>证明NSP没用；使用动态掩码，即在每次输入时随机生成Mask；认为模型训练不充分（undertrained）。</td>
<td>曾经是是工业界做分类、实体识别任务的首选模型</td>
</tr>
<tr>
<td>ALBERT</td>
<td>Google，2019</td>
<td>12层Encoder的参数全部共享；因式分解Embedding，从而减少了嵌入层的参数量。</td>
<td>节省显存</td>
</tr>
<tr>
<td>XLNet</td>
<td>Google+CMU，2019</td>
<td>排列语言建模（Permutation Language Modeling，PLM）：不使用<code>[MASK]</code>，用数学公式随机打乱输入序列的顺序，然后像GPT一样预测下一个词。并引入Transformer-XL的<strong>长距离记忆机制</strong>。</td>
<td>训练过于复杂</td>
</tr>
<tr>
<td>ELECTRA</td>
<td>Google+Stanford，2020</td>
<td>模仿GAN训练：RTD（Replaced Token Detection）任务，生成器负责填充Mask，判别器负责判断每一个词是“原装的”还是“被篡改过的”。</td>
<td>训练效率高</td>
</tr>
</tbody>
</table>
<h2 id="3-2-BART">3.2 BART</h2>
<p>Facebook AI于2019年提出<strong>BART</strong>，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.13461">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a>。结构仍然基于Transformer的Encoder-Decoder模型。该模型采用去噪自编码（Denoising Autoencoder）进行预训练，即给输入文本加各种噪音（打乱句子顺序、删除词、遮盖词），然后让模型还原出原始文本。在<strong>文本摘要</strong>领域是冠军。</p>

    </div>
    <script>
    document.addEventListener('DOMContentLoaded', () => {
        const codeBlocks = document.querySelectorAll('figure.highlight');
    
        codeBlocks.forEach(block => {
            // 1. 获取语言标签
            const lang = block.classList.contains('highlight') ? 
                         block.classList[1] : '';
    
            // 2. 创建一个容器来包裹语言标签和复制按钮
            const toolbar = document.createElement('div');
            toolbar.className = 'highlight-toolbar';
    
            // 3. 创建语言标签
            if (lang) {
                const langLabel = document.createElement('span');
                langLabel.className = 'highlight-lang';
                langLabel.textContent = lang.toUpperCase();
                toolbar.appendChild(langLabel);
            }
    
            // 4. 创建复制按钮
            const copyButton = document.createElement('button');
            copyButton.className = 'highlight-copy-btn';
            copyButton.textContent = '复制';
            
            copyButton.addEventListener('click', () => {
                // 优先选择 .code 元素（针对有行号的表格），如果没有则选择 pre 元素
                const codeElement = block.querySelector('.code') || block.querySelector('pre');
                if (codeElement) {
                    navigator.clipboard.writeText(codeElement.textContent).then(() => {
                        copyButton.textContent = '已复制!';
                        setTimeout(() => {
                            copyButton.textContent = '复制';
                        }, 2000);
                    }).catch(err => {
                        console.error('复制失败: ', err);
                    });
                }
            });
            toolbar.appendChild(copyButton);
    
            // 5. 将工具栏插入到代码块中
            block.appendChild(toolbar);
        });
    });
</script>
  </article>
</div>

    </div>
    
<script src="/js/script.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>