<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2.2 RAG | Hexo</title>
    
<link rel="stylesheet" href="/css/style.css">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          TeX: {
            extensions: ["amsmath.js", "cancel.js"],
          }
        });
    </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"/>
<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div class="container">
        <header class="post-navbar">
    <div class="nav-left">
        <div class="nav-item search-trigger">
            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24"><path fill="currentColor" d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0 0 20 11c0-4.97-4.03-9-9-9s-9 4.03-9 9 4.03 9 9 9c2.49 0 4.74-1.01 6.32-2.67l3.68 3.68a1 1 0 0 0 1.42 0a1 1 0 0 0 0-1.42ZM4 11a7 7 0 1 1 14 0a7 7 0 0 1-14 0Z"/></svg>
            <span style="font-size: small;">搜索...</span>
            <span class="search-shortcut" style="font-size: small;">Ctrl K</span>
        </div>
    </div>
    <div style="width: 10px;"></div>
    <div class="nav-right">
        <div class="nav-item theme-switcher">
            <svg class="sun" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 12a4 4 0 1 0 8 0a4 4 0 1 0-8 0m-5 0h1m8-7V4m-5.6 1.4L4.8 6.8M16 17.2l1.4 1.4M20 12h1m-6-5.2l1.4-1.4M12 21v-1"/></svg>
            <svg class="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3a6 6 0 0 0 9 9a9 9 0 1 1-9-9Z"/></svg>
        </div>
    </div>
</header>

<div id="search-modal" class="modal-overlay" style="display: none;">
    <div class="modal-content">
        <input type="text" id="search-input" placeholder="输入关键字搜索...">
        <div id="search-results"></div>
    </div>
</div>
<div class="post-layout">
  <aside class="post-sidebar">
    
    
      
      <nav>
          <div class="sidebar-top">
              <a href="/" class="back-to-home-link" style="display: flex; align-items: center">
                  <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 20 20"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="m15 18l-6-6l6-6"/></svg>
                  <span>返回首页</span>
              </a>
              <h3>大语言模型</h3>
          </div>
          <ul>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">1 大模型微调</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/2025/08/19/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/1-%E5%BE%AE%E8%B0%83%E5%AE%9E%E8%B7%B5/">1.1 微调实践</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
                  <li class="sidebar-group">
                      <details open>
                          <summary>
                              <svg class="sidebar-arrow" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"><path fill="currentColor" d="m15.5 12.001l-5.75 5.75c-.15.15-.325.225-.525.225s-.375-.075-.525-.225c-.3-.3-.3-.775 0-1.075l5.225-5.25l-5.225-5.25c-.3-.3-.3-.775 0-1.075s.775-.3 1.075 0l5.75 5.75c.15.15.225.325.225.525s-.075.375-.225.525Z"/></svg>
                              <span class="sidebar-summary-text">2 应用开发</span>
                          </summary>
                          <div class="details-content">
                              <ul>
                                  
                                      <li class="sidebar-item ">
                                          <a href="/2025/08/19/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/2.2-MCP/">2.1 MCP</a>
                                      </li>
                                  
                                      <li class="sidebar-item active">
                                          <a href="/2025/08/19/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/2.1-RAG/">2.2 RAG</a>
                                      </li>
                                  
                              </ul>
                          </div>
                      </details>
                  </li>
              
          </ul>
      </nav>
    
  </aside>

  <article class="post-content">
    <h1>2.2 RAG</h1>
    <div class="post-meta">
        <time class="post-date-tag" datetime="2025-08-18T16:00:00.000Z">
            发布于: 2025-08-19
        </time>
    </div>
    <div class="post-body">
        <h2 id="1-检索增强生成">1 检索增强生成</h2>
<ul>
<li><strong>R</strong>etrieval-<strong>A</strong>ugmented <strong>G</strong>eneration</li>
</ul>
<p>你正在看一份文档敲代码，毫无疑问地，你遇到了问题，现在你将这个问题抛给AI大模型，即便大模型从来都没见过你的这份文档，但大模型仍然会一本正经地胡说八道，出现了<strong>幻觉</strong>。没有得到你想要的答案，于是你将问题和答案一起抛给大模型，这次就给出了正确的回答。</p>
<p>问题似乎解决了，但你的文档会变得越来越大，关键的信息只会藏在其中的某一角落里，大模型看了整个文档之后注意力难免也会涣散。那么能不能把文档中相关的部分发给大模型呢？答案是肯定的，这就是RAG想要解决的问题。现在的难题就是：<u>如何判断文档中的某一部分和你的问题是否相关</u>。</p>
<h2 id="2-嵌入模型">2 嵌入模型</h2>
<ul>
<li>Embedding Model，输入一段文字，输出一段<strong>固定长度</strong>的向量，如OpenAI的text-embedding-3-small模型输出的数组长度为1536，对应的large模型就是small模型的2倍。无论你的信息有多长，信息都被压缩到固定维度的向量中了。最后，可以通过计算向量之间的距离来判断两个信息之间的关联度。向量空间的维度足够大，能够将各种信息的向量距离区分开。</li>
<li>接下来，你的问题经过Embedding模型的压缩，被映射到向量空间上的某一个位置上，得到距离你的问题最近的几段文字当作上下文，和你的问题一起发给大模型。于是大模型看到的就是和你的问题强相关的信息了。</li>
</ul>
<p>我们的目的是找到整个文档中与你的问题相关的部分，那么文档该如何切分成更小的单元呢？可以按字数、段落、句子来切分。经过文字分块（chunking）之后，每一块文字都做一个Embedding，并将得到的向量与文字片段对应起来。</p>
<ul>
<li>现成的分块算法：LangChain中的Recursive Character Text Splitter</li>
</ul>
<p>为了防止AI偷偷用它的知识库，准备一篇不同寻常的文档<code>file.md</code>，该文档是标准的markdown格式。下面就专门对该文档做一个简单的切分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Literal</span>, <span class="hljs-type">Any</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_data</span>() -&gt; <span class="hljs-built_in">str</span>:<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;file.md&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">return</span> f.read()<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_chunks</span>() -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>]:<br>    content: <span class="hljs-built_in">str</span> = read_data()<br>    chunks: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>] = content.split(<span class="hljs-string">&#x27;\n\n&#x27;</span>)<br><br>    result: <span class="hljs-built_in">list</span>[<span class="hljs-type">Any</span>] = []<br>    header: <span class="hljs-type">Literal</span>[<span class="hljs-string">&#x27;&#x27;</span>] = <span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> chunks:<br>        <span class="hljs-keyword">if</span> c.startswith(<span class="hljs-string">&quot;#&quot;</span>):<br>            header += <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;c&#125;</span>\n&quot;</span><br>        <span class="hljs-keyword">else</span>:<br>            result.append(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;header&#125;</span><span class="hljs-subst">&#123;c&#125;</span>&#x27;</span>)<br>            header: <span class="hljs-type">Literal</span>[<span class="hljs-string">&#x27;&#x27;</span>] = <span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> result<br></code></pre></td></tr></table></figure>
<p>接下来以google的embedding模型来写一个嵌入方法。考虑到问题和内容的形式差别会比较大，嵌入距离就没那么近，因此google的模型有点特别（qwen3也是这样），分<strong>存储</strong>和<strong>查询</strong>两种方式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> chromadb<br><span class="hljs-keyword">from</span> google <span class="hljs-keyword">import</span> genai<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span>, <span class="hljs-type">Any</span><br><br>google_client = genai.Client()<br>EMBEDDING_MODEL = <span class="hljs-string">&#x27;gemini-embedding-exp-03-07&#x27;</span><br>LLM_MODEL = <span class="hljs-string">&#x27;gemini-2.5-flash-preview-05-20&#x27;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">embed</span>(<span class="hljs-params">text: <span class="hljs-built_in">str</span>, store: <span class="hljs-built_in">bool</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]:<br>    result: <span class="hljs-type">Any</span> = google_client.models.embed_content(<br>        model=EMBEDDING_MODEL,<br>        contents=text,<br>        config=&#123;<br>            <span class="hljs-string">&quot;task_type&quot;</span>: <span class="hljs-string">&quot;RETRIEVAL_DOCUMENT&quot;</span> <span class="hljs-keyword">if</span> store <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;RETRIEVAL_QUERY&quot;</span><br>        &#125;<br>    )<br>    <span class="hljs-comment"># 懒得处理了</span><br>    <span class="hljs-keyword">assert</span> result.embeddings<br>    <span class="hljs-keyword">assert</span> result.embeddings[<span class="hljs-number">0</span>].values<br>    <span class="hljs-keyword">return</span> result.embeddings[<span class="hljs-number">0</span>].values<br></code></pre></td></tr></table></figure>
<p>传统的数据库用键值对的方式来存储这种对应关系，但我们并不是想通过精确的key值来查找对应的向量或文本，而是要找距离问题向量最近的几个向量对应的文本，这时候传统的数据库就不行了。针对这种场景，专门设计了向量数据库。</p>
<h2 id="3-向量数据库">3 向量数据库</h2>
<ul>
<li>存储结构当然是嵌入向量和对应的文本</li>
<li>常见的有Pinecone，chromaDB，postgreSQL（配合pgvector插件）等</li>
</ul>
<p>下面构建<code>chromadb</code>一个实例来存储嵌入向量-文本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">chromadb_client: ClientAPI = chromadb.PersistentClient(<span class="hljs-string">&#x27;./数据库名.db&#x27;</span>)<br>chromadb_collection: Collection = chromadb_client.get_or_create_collection(<span class="hljs-string">&quot;数据表的名字&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>接下来写一个函数对每个chunk做一个Embedding</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_db</span>() -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-keyword">for</span> idx, c <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(get_chunks()):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Process <span class="hljs-subst">&#123;c&#125;</span>&#x27;</span>)<br>        embedding: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>] = embed(c, store=<span class="hljs-literal">True</span>)<br>        chromadb_collection.upsert(<br>            ids=<span class="hljs-built_in">str</span>(idx),	<span class="hljs-comment"># 要求为每条数据提供一个id（没啥用）</span><br>            documents=c,<br>            embeddings=embedding<br>        )<br></code></pre></td></tr></table></figure>
<p>最后完成查询函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">query_db</span>(<span class="hljs-params">question: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>]:<br>    question_embedding: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">float</span>] = embed(question, store=<span class="hljs-literal">False</span>)<br>    result: QueryResult = chromadb_collection.query(<br>    	query_embeddings=question_embedding,<br>        n_results=<span class="hljs-number">5</span><br>    )<br>    <span class="hljs-keyword">assert</span> result[<span class="hljs-string">&#x27;documents&#x27;</span>]<br>    <span class="hljs-keyword">return</span> result[<span class="hljs-string">&#x27;documents&#x27;</span>][<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure>
<p>将信息拼凑成一个prompt给大模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">question = <span class="hljs-string">&quot;xxxx&quot;</span><br>chunks: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>] = query_db(question)<br>prompt = <span class="hljs-string">&quot;请根据上下文回答用户的问题：\n&quot;</span><br>prompt += <span class="hljs-string">f&quot;问题：<span class="hljs-subst">&#123;question&#125;</span>\n&quot;</span><br>prompt += <span class="hljs-string">f&quot;上下文：\n&quot;</span><br><span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> chunks:<br>    prompt += <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;c&#125;</span>\n&quot;</span><br>    prompt += <span class="hljs-string">&quot;========&quot;</span><br>    <br><span class="hljs-comment"># 发给Gemini Flash 2.5</span><br>result: GenerateContentResponse = google_client.models.generate_content(<br>    model=LLM_MODEL,<br>    contents=prompt<br>)<br></code></pre></td></tr></table></figure>
<h2 id="4-总结">4 总结</h2>
<p>RAG并不是一个完美的框架</p>
<ul>
<li>文本的结构不同，文本怎么分块，无法适配所有的场景。比如将两句相关的句子切分了，导致二者不关联。</li>
<li>没有全局的视角，比如我要统计一篇文档中出现了多少个某单词，这种整体都沾边的问题RAG是无法处理的</li>
</ul>
<p>改进：让大模型参与分块的过程，根据语义自动判断应该在哪里断开…</p>
<p>笔记整理自</p>

    </div>
    <script>
    document.addEventListener('DOMContentLoaded', () => {
        const codeBlocks = document.querySelectorAll('figure.highlight');
    
        codeBlocks.forEach(block => {
            // 1. 获取语言标签
            const lang = block.classList.contains('highlight') ? 
                         block.classList[1] : '';
    
            // 2. 创建一个容器来包裹语言标签和复制按钮
            const toolbar = document.createElement('div');
            toolbar.className = 'highlight-toolbar';
    
            // 3. 创建语言标签
            if (lang) {
                const langLabel = document.createElement('span');
                langLabel.className = 'highlight-lang';
                langLabel.textContent = lang.toUpperCase();
                toolbar.appendChild(langLabel);
            }
    
            // 4. 创建复制按钮
            const copyButton = document.createElement('button');
            copyButton.className = 'highlight-copy-btn';
            copyButton.textContent = '复制';
            
            copyButton.addEventListener('click', () => {
                const code = block.querySelector('pre');
                if (code) {
                    navigator.clipboard.writeText(code.textContent).then(() => {
                        copyButton.textContent = '已复制!';
                        setTimeout(() => {
                            copyButton.textContent = '复制';
                        }, 2000);
                    }).catch(err => {
                        console.error('复制失败: ', err);
                    });
                }
            });
            toolbar.appendChild(copyButton);
    
            // 5. 将工具栏插入到代码块中
            block.appendChild(toolbar);
        });
    });
</script>
  </article>
</div>
    </div>
    
<script src="/js/script.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>